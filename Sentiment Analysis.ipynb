{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy # Load the language\n",
    "import json\n",
    "# Install bs4: sudo apt-get install python3-bs4\n",
    "nlp = spacy.load('en_core_web_lg') # Use the following to install from command line: python3 -m spacy download en_core_web_lg\n",
    "# Change the below to the location of the dictionary file\n",
    "dict_file = '/home/caspar/Documents/Data Science/bbc_sentiment_analysis/senticnet.xml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Use this to compare how long code takes\n",
    "\n",
    "def time_execution(code):\n",
    "    start = time.clock()\n",
    "    code_part = eval('code')\n",
    "    stop = time.clock()\n",
    "    run_time = stop - start\n",
    "    return run_time\n",
    "\n",
    "# A container for Spacy to analyse sentiment\n",
    "sentiment_words = []\n",
    "\n",
    "# Word identified that I don't want to be included\n",
    "stop_words = []\n",
    "\n",
    "\n",
    "a = 'https://www.telegraph.co.uk/'\n",
    "\n",
    "c = 'https://www.nytimes.com/'\n",
    "\n",
    "b = 'https://www.bbc.co.uk/news'\n",
    "\n",
    "def get_page(page_address):\n",
    "    page = requests.get(a)\n",
    "    # Create a BeautifulSoup object\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_headlines(soup):\n",
    "    all_headlines = soup.find_all(['h1','h2','h3','h4','h5'])\n",
    "    return all_headlines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current date and time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    " \n",
    "\n",
    "# dd/mm/YY H:M:S\n",
    "dt_string = now.strftime(\"%d/%m/%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get full page html\n",
    "\n",
    "soup = get_page(a)\n",
    "\n",
    "# Get the headlines including html tags\n",
    "\n",
    "all_headlines = get_headlines(soup)\n",
    "\n",
    "# Get just a list of headline strings\n",
    "\n",
    "headline_list = []\n",
    "for i in all_headlines:\n",
    "    if i.text not in headline_list and (i.text != ''):\n",
    "        headline_list.append(i.text)\n",
    "        \n",
    "# Remove BBC links\n",
    "if a == 'https://www.bbc.co.uk/news':\n",
    "    headline_list = headline_list[5:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words and punctuation and put remaining words in sentiment_words\n",
    "\n",
    "# Create a list of individual words in headline and put into this:\n",
    "individualized_headline_words = []\n",
    "\n",
    "for headline in headline_list:\n",
    "    doc = nlp(headline)\n",
    "    individualized_headline_words.append(headline.split())\n",
    "    sentiment_words.append([token.lemma_ for token in doc if not token.is_stop if not token.lemma_ in stop_words if token.is_punct == False])\n",
    "\n",
    "# Create a large list for all words in all headlines\n",
    "all_words = [] # e.f. ['virus', 'update', 'UK', 'world', 'live']\n",
    "\n",
    "# Now populate this list\n",
    "for headline in sentiment_words:\n",
    "    for word in headline:\n",
    "        all_words.append(word)\n",
    "\n",
    "# Define a dictionary file with word sentiments\n",
    "import xml.etree.ElementTree as ET\n",
    "import pprint\n",
    "\n",
    "tree = ET.parse(dict_file)\n",
    "root = tree.getroot()\n",
    "\n",
    "i = 0\n",
    "data=[] # Data holds a tuple of the word and then its score\n",
    "while i < len(root):\n",
    "    data.append((root[i][1].text,root[i][2].text))\n",
    "    i+=1\n",
    "\n",
    "data_list = list(data)\n",
    "# data_list contains ('word,'0.1') this is the one with the score\n",
    "# unique_words ('word','word') this is taken from the bbc\n",
    "\n",
    "### Convert data_list to a dictionary\n",
    "\n",
    "polarity_dictionary = {}\n",
    "\n",
    "for i in data_list:\n",
    "    key_word = i[0]\n",
    "    polarity_value = i[1]\n",
    "    polarity_dictionary[key_word] = float(polarity_value)\n",
    "\n",
    "score_dict = [] # Has all the bbc words and scores in the format [('Saturday', 0.935),('abuse', -0.684)]\n",
    "zero_words = [] # Has all the words without a polarity rating as a list of strings ['virus','update','UK']\n",
    "\n",
    "for bbc_word in all_words:\n",
    "    if bbc_word in polarity_dictionary:\n",
    "        score_dict.append(polarity_dictionary[bbc_word])\n",
    "    else:\n",
    "        zero_words.append(bbc_word)\n",
    "\n",
    "# Takes a python dictionary and turns it into a json file\n",
    "\n",
    "def dict_to_json(dict,file_name):\n",
    "    # dict should be a python dictionary and file_name in the format \"my_file.json\",\n",
    "    # which will be created when function is run.\n",
    "    a_file = open(file_name, \"w\")\n",
    "    json.dump(dict, a_file, sort_keys=True, indent=4, separators=(',', ': '))\n",
    "    a_file.close()\n",
    "\n",
    "# Takes a json file and turns it into a python dictionary\n",
    "\n",
    "def json_to_dic(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        this_dict = json.load(f)\n",
    "    return this_dict\n",
    "   \n",
    "result = round((sum(score_dict)/len(score_dict)),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the current json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file \n",
    "with open('/home/caspar/Documents/Data Science/bbc_sentiment_analysis/headline_scores.json') as json_file: \n",
    "    data = json.load(json_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_serial contains the last serial no. in the dictionary\n",
    "counter = 0\n",
    "for i in reversed(data):\n",
    "    while counter < 1:\n",
    "        last_serial = (i)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working code\n",
    "# Create a variable for the polarity of a particular headline\n",
    "headline_score = 0\n",
    "serial_no = 0\n",
    "# Create a list to contain the full sentence headlines and their sentiment scores\n",
    "new_headline_list = []\n",
    "headline_dict = data #{} #json_to_dic('/home/caspar/Documents/Data Science/bbc_sentiment_analysis/headline_scores.json')\n",
    "\n",
    "\n",
    "#start the count at the end of the last dictionary\n",
    "headline_counter = int(last_serial)\n",
    "\n",
    "for headline in headline_list:\n",
    "    # Check if headline is already in the headline_list\n",
    "    if headline not in new_headline_list:\n",
    "        \n",
    "        # Add headline score to a list\n",
    "        single_word_sentiment = []\n",
    "        \n",
    "        # Check to see if each word is in polarity dictionary\n",
    "        for bbc_word in headline.split():\n",
    "            \n",
    "            # If bbc_word is in the polarity dictionary\n",
    "            if bbc_word in polarity_dictionary:\n",
    "                # Update the single_word_sentiment list with that word score\n",
    "                single_word_sentiment.append(polarity_dictionary[bbc_word])\n",
    "                \n",
    "        # If the polarity score is above zero for the entire headline\n",
    "        if len(single_word_sentiment) > 0:\n",
    "            \n",
    "            # Work out the headline score by averaging over all words with a score\n",
    "            headline_score = round(sum(single_word_sentiment)/len(single_word_sentiment),2)\n",
    "        else:\n",
    "            headline_score = 0\n",
    "            \n",
    "        # Update the new_headline_list with the full sentence headline as well as its score.\n",
    "        # The last_serial is subtracted to ensure continuation of serial numbers in current json file.\n",
    "        new_headline_list.append((headline_list[headline_counter-int(last_serial)],headline_score))\n",
    "        \n",
    "        # Add full headline, time of the pull and sentiment score to a dict.\n",
    "        headline_dict[headline_counter+1] = (new_headline_list[headline_counter-int(last_serial)],dt_string,headline_score,0)\n",
    "        \n",
    "        # Go to next headline\n",
    "        headline_counter += 1\n",
    "        \n",
    "        # Bump serial no. by 1\n",
    "        serial_no += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into JSON:\n",
    "#y = json.dumps(headline_dict)\n",
    "   \n",
    "with open('/home/caspar/Documents/Data Science/bbc_sentiment_analysis/headline_scores.json', 'w') as outfile:\n",
    "    json.dump(headline_dict, outfile, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
